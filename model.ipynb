{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Specify the file path in your Google Drive\n",
    "file_path = '/content/drive/StonyBrook/Conversational MBTI Classifier/Train.csv' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "\n",
    "TEST_PATH = \"testdata.csv\"\n",
    "TRAIN_PATH= \"traindata.csv\"\n",
    "EVAL_PATH = \"evaldata.csv\"\n",
    "SAVE_PATH= \"data/Save\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(path):\n",
    "    dataset = pd.read_csv(path)\n",
    "    return dataset\n",
    "train_data = import_data(TRAIN_PATH)\n",
    "test_data = import_data(TEST_PATH)\n",
    "eval_data = import_data(EVAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Big5Classifier:\n",
    "    def __init__(self, model_name='distilbert-base-uncased', num_classes=2):\n",
    "        # Initialize models here\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = num_classes)\n",
    "\n",
    "    def get_tokenizer_and_model(self):\n",
    "        return self.model, self.tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader(Dataset):\n",
    "    #cateogry can be \"OPN\", \"CON\", \"EXT\", \"AGR\", \"NEU\"\n",
    "    def __init__(self, data, category, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.category = category\n",
    "        self.questionMap =  {\"OPN\":\"This is a question about openness: Describe a time when you tried something completely newâ€”whether it was a different activity, way of thinking, or environment. What motivated you to try it, and how did you feel about the experience afterward? \",\n",
    "                \"CON\":\"This is a question about conscientiousness: Think of a goal you set for yourself that required sustained effort over time. How did you manage your time and resources to stay on track, and what strategies helped you stay committed, even when challenges came up? What did you find challenging or rewarding about the experience?\",\n",
    "                \"EXT\":\"This is a question about extraversion: Recall a memorable social experience that either energized you or left you feeling drained. What do you think made the interaction fulfilling or draining? How did it shape your understanding of your social preferences or needs? \",\n",
    "                \"AGR\":\"This is a question about agreeableness: Describe a situation where you found yourself in disagreement with someone. How did you handle the situation, and what were your priorities in resolving or understanding the conflict? \",\n",
    "                \"NEU\":\"This is a question about neuroticism: Think of a time when you felt particularly stressed or anxious. How did you respond initially, and what steps did you take to manage your emotions and approach the situation constructively? \"\n",
    "               }\n",
    "    \n",
    "    def tokenize_data(self):\n",
    "        print(\"Processing data\")\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        label_dict = {'y': 1, 'n': 0}\n",
    "\n",
    "        answers = self.data['essay'].to_list() \n",
    "        label_list = self.data['decision'].to_list()\n",
    "\n",
    "        for (answer, decision) in tqdm(zip(answer, label_list), total = len(answers)):\n",
    "            #concatenate the question asked before the answer.\n",
    "            qa_concat = f\"{self.questionMap[self.category]} [SEP] {answer} \"\n",
    "            encoded_text = self.tokenizer.encode(qa_concat, max_length = 400, truncation = True)\n",
    "            tokens.append(torch.tensor(encoded_text))\n",
    "            labels.append(label_dict)[decision]\n",
    "        \n",
    "        tokens = pad_sequence(tokens, batch_first = True)\n",
    "        labels = torch.tensor(labels)\n",
    "        return TensorDataset(tokens, labels)\n",
    "    \n",
    "    def get_data_loaders(self, batch_size = 32, shuffle = True):\n",
    "        processed_dataset = self.tokenize_data()\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            processed_dataset,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_data, eval_data, batch_size, epochs, optimizer, criterion, device, savepath, training_type):\n",
    "        \"\"\"\n",
    "        Initializes the Trainer class with training and evaluation parameters.\n",
    "        Args:\n",
    "        - model: PyTorch model to be trained (e.g., DistilBERT).\n",
    "        - train_data: Training dataset.\n",
    "        - eval_data: Evaluation dataset.\n",
    "        - batch_size: Batch size for DataLoader.\n",
    "        - epochs: Number of training epochs.\n",
    "        - optimizer: Optimizer (e.g., Adam).\n",
    "        - criterion: Loss function (e.g., CrossEntropyLoss).\n",
    "        - device: Device to run the model on (e.g., 'cuda' or 'cpu').\n",
    "        - savepath: Path to save the model after training.\n",
    "        - training_type: Type of training (\"fully_frozen\", \"top_4_training\", \"bottom_4_training\", \"all_training\").\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.savepath = savepath\n",
    "        self.training_type = training_type\n",
    "\n",
    "        # Create DataLoaders from datasets\n",
    "        self.train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        self.eval_dataloader = DataLoader(eval_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Send model to device\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Set the training parameters based on the training type\n",
    "        self.set_training_parameters()\n",
    "\n",
    "    def set_training_parameters(self):\n",
    "        \"\"\"\n",
    "        Freezes or unfreezes model parameters based on the specified training type.\n",
    "        \"\"\"\n",
    "        # Freeze all parameters by default\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if self.training_type == \"fully_frozen\":\n",
    "            # Only the classifier head is trainable\n",
    "            for param in self.model.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif self.training_type == \"top_4_training\":\n",
    "            # Unfreeze the top 4 layers of the transformer and the classifier\n",
    "            for layer in range(2, 6):  # Loop through top 4 layers\n",
    "                for param in self.model.distilbert.transformer.layer[layer].parameters():\n",
    "                    param.requires_grad = True\n",
    "            for param in self.model.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif self.training_type == \"bottom_4_training\":\n",
    "            # Unfreeze the bottom 4 layers of the transformer and the classifier\n",
    "            for layer in range(0, 4):  # Loop through bottom 4 layers\n",
    "                for param in self.model.distilbert.transformer.layer[layer].parameters():\n",
    "                    param.requires_grad = True\n",
    "            for param in self.model.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif self.training_type == \"all_training\":\n",
    "            # Unfreeze all layers\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        print(f\"Training type set to: {self.training_type}\")\n",
    "\n",
    "    def get_performance_metrics(self, preds, labels):\n",
    "        \"\"\"\n",
    "        Calculate performance metrics: accuracy, precision, recall, and F1-score.\n",
    "        Args:\n",
    "        - preds: Predictions from the model.\n",
    "        - labels: Ground truth labels.\n",
    "        Returns:\n",
    "        - metrics: Dictionary containing accuracy, precision, recall, and F1-score.\n",
    "        \"\"\"\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(labels_flat, pred_flat),\n",
    "            \"precision\": precision_score(labels_flat, pred_flat, zero_division=0, average=\"weighted\"),\n",
    "            \"recall\": recall_score(labels_flat, pred_flat, zero_division=0, average=\"weighted\"),\n",
    "            \"f1\": f1_score(labels_flat, pred_flat, zero_division=0, average=\"weighted\")\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the model and evaluates on validation data after each epoch.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}\")\n",
    "            self.model.train()  # Set model to training mode\n",
    "            total_loss = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            for batch in self.train_dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                logits = outputs[0]  # Assuming logits are the first output\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = self.criterion(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Collect predictions and labels for metrics\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                labels = labels.cpu().numpy()\n",
    "\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels)\n",
    "\n",
    "            avg_loss = total_loss / len(self.train_dataloader)\n",
    "            print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Calculate and print training metrics\n",
    "            metrics = self.get_performance_metrics(np.array(all_preds), np.array(all_labels))\n",
    "            print(f\"Training Metrics: {metrics}\")\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            print(\"Evaluating on validation set...\")\n",
    "            self.evaluate()\n",
    "\n",
    "            # Save model (optional, can save after every epoch or after training)\n",
    "            torch.save(self.model.state_dict(), self.savepath)\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the validation dataset.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in self.eval_dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                logits = outputs[0]  # Assuming logits are the first output\n",
    "\n",
    "                # Collect predictions and labels for metrics\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                labels = labels.cpu().numpy()\n",
    "\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels)\n",
    "\n",
    "        # Calculate and print evaluation metrics\n",
    "        metrics = self.get_performance_metrics(np.array(all_preds), np.array(all_labels))\n",
    "        print(f\"Evaluation Metrics: {metrics}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['batch_size']=BATCH_SIZE\n",
    "params['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "params['train_data']= \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
