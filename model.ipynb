{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Specify the file path in your Google Drive\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Specify the file path in your Google Drive\n",
    "file_path = '/content/drive/StonyBrook/Conversational MBTI Classifier/Train.csv' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "\n",
    "TEST_PATH = \"testdata.csv\"\n",
    "TRAIN_PATH= \"traindata.csv\"\n",
    "EVAL_PATH = \"evaldata.csv\"\n",
    "SAVE_PATH= \"models/Big5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(path):\n",
    "    dataset = pd.read_csv(path)\n",
    "    return dataset\n",
    "train_data = import_data(TRAIN_PATH)\n",
    "test_data = import_data(TEST_PATH)\n",
    "eval_data = import_data(EVAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Big5Classifier:\n",
    "    def __init__(self, model_name='distilbert-base-uncased', num_classes=2):\n",
    "        # Initialize models here\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = num_classes)\n",
    "\n",
    "    def get_tokenizer_and_model(self):\n",
    "        return self.model, self.tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_split(data, big5category):\n",
    "    text_column = f\"text{big5category[0]}\"\n",
    "    category_column = f\"c{big5category}\"\n",
    "\n",
    "    # Check if the required columns exist\n",
    "    if text_column in data.columns and category_column in data.columns:\n",
    "        return data[[text_column, category_column]].rename(\n",
    "            columns={text_column: \"essay\", category_column: \"decision\"}\n",
    "        )    \n",
    "    else:\n",
    "        raise ValueError(f\"Columns for category '{big5category}' are not present in the dataset.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader(Dataset):\n",
    "    #cateogry can be \"OPN\", \"CON\", \"EXT\", \"AGR\", \"NEU\"\n",
    "    def __init__(self, data, big5category, tokenizer):\n",
    "        self.data = get_dataset_split(data, big5category)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.big5category = big5category\n",
    "        self.questionMap =  {\"OPN\":\"This is a question about openness: Describe a time when you tried something completely newâ€”whether it was a different activity, way of thinking, or environment. What motivated you to try it, and how did you feel about the experience afterward? \",\n",
    "                \"CON\":\"This is a question about conscientiousness: Think of a goal you set for yourself that required sustained effort over time. How did you manage your time and resources to stay on track, and what strategies helped you stay committed, even when challenges came up? What did you find challenging or rewarding about the experience?\",\n",
    "                \"EXT\":\"This is a question about extraversion: Recall a memorable social experience that either energized you or left you feeling drained. What do you think made the interaction fulfilling or draining? How did it shape your understanding of your social preferences or needs? \",\n",
    "                \"AGR\":\"This is a question about agreeableness: Describe a situation where you found yourself in disagreement with someone. How did you handle the situation, and what were your priorities in resolving or understanding the conflict? \",\n",
    "                \"NEU\":\"This is a question about neuroticism: Think of a time when you felt particularly stressed or anxious. How did you respond initially, and what steps did you take to manage your emotions and approach the situation constructively? \"\n",
    "               }\n",
    "    \n",
    "    def tokenize_data(self):\n",
    "        print(\"Processing data\")\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        label_dict = {'y': 1, 'n': 0}\n",
    "\n",
    "        answers = self.data['essay'].to_list() \n",
    "        label_list = self.data['decision'].to_list()\n",
    "\n",
    "        for (answer, decision) in tqdm(zip(answers, label_list), total = len(answers)):\n",
    "            #concatenate the question asked before the answer.\n",
    "            qa_concat = f\"{self.questionMap[self.big5category]} [SEP] {answer} \"\n",
    "            encoded_text = self.tokenizer.encode(qa_concat, max_length = 400, truncation = True)\n",
    "            tokens.append(torch.tensor(encoded_text))\n",
    "            labels.append(label_dict[decision])\n",
    "        \n",
    "        tokens = pad_sequence(tokens, batch_first = True)\n",
    "        labels = torch.tensor(labels)\n",
    "        return TensorDataset(tokens, labels)\n",
    "    \n",
    "    def get_data_loaders(self, batch_size = 32, shuffle = True):\n",
    "        processed_dataset = self.tokenize_data()\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            processed_dataset,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, big5category, train_data, eval_data, batch_size, epochs, device, save_path, training_type):\n",
    "        \"\"\"\n",
    "        Initializes the Trainer class with training and evaluation parameters.\n",
    "        Args:\n",
    "        - model: PyTorch model to be trained (e.g., DistilBERT).\n",
    "        - train_data: Training dataset.\n",
    "        - eval_data: Evaluation dataset.\n",
    "        - batch_size: Batch size for DataLoader.\n",
    "        - epochs: Number of training epochs.\n",
    "        - optimizer: Optimizer (e.g., Adam).\n",
    "        - criterion: Loss function (e.g., CrossEntropyLoss).\n",
    "        - device: Device to run the model on (e.g., 'cuda' or 'cpu').\n",
    "        - savepath: Path to save the model after training.\n",
    "        - training_type: Type of training (\"fully_frozen\", \"top_4_training\", \"bottom_4_training\", \"all_training\").\n",
    "        \"\"\"\n",
    "        transformer = Big5Classifier()\n",
    "        self.model, self.tokenizer = transformer.get_tokenizer_and_model()\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.save_path = save_path\n",
    "        self.training_type = training_type\n",
    "\n",
    "        # Create DataLoaders from datasets\n",
    "        self.train_dataset = DatasetLoader(train_data, big5category, self.tokenizer)\n",
    "        self.eval_dataset = DatasetLoader(eval_data, big5category, self.tokenizer)\n",
    "\n",
    "        # Send model to device\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Set the training parameters based on the training type\n",
    "        self.set_training_parameters()\n",
    "\n",
    "    def set_training_parameters(self):\n",
    "        \"\"\"\n",
    "        Freezes or unfreezes model parameters based on the specified training type.\n",
    "        \"\"\"\n",
    "        # Freeze all parameters by default\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if self.training_type == \"fully_frozen\":\n",
    "            # Only the classifier head is trainable\n",
    "            for param in self.model.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif self.training_type == \"top_4_training\":\n",
    "            # Unfreeze the top 4 layers of the transformer and the classifier\n",
    "            for layer in range(2, 6):  # Loop through top 4 layers\n",
    "                for param in self.model.distilbert.transformer.layer[layer].parameters():\n",
    "                    param.requires_grad = True\n",
    "            for param in self.model.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif self.training_type == \"bottom_4_training\":\n",
    "            # Unfreeze the bottom 4 layers of the transformer and the classifier\n",
    "            for layer in range(0, 4):  # Loop through bottom 4 layers\n",
    "                for param in self.model.distilbert.transformer.layer[layer].parameters():\n",
    "                    param.requires_grad = True\n",
    "            for param in self.model.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif self.training_type == \"all_training\":\n",
    "            # Unfreeze all layers\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        print(f\"Training type set to: {self.training_type}\")\n",
    "\n",
    "    def get_performance_metrics(self, preds, labels):\n",
    "        \"\"\"\n",
    "        Calculate performance metrics: accuracy, precision, recall, and F1-score.\n",
    "        Args:\n",
    "        - preds: Predictions from the model.\n",
    "        - labels: Ground truth labels.\n",
    "        Returns:\n",
    "        - metrics: Dictionary containing accuracy, precision, recall, and F1-score.\n",
    "        \"\"\"\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "\n",
    "        accuracy = accuracy_score(labels_flat, pred_flat)\n",
    "        precision = precision_score(labels_flat, pred_flat, zero_division=0, average=\"weighted\")\n",
    "        recall = recall_score(labels_flat, pred_flat, zero_division=0, average=\"weighted\")\n",
    "        f1 = f1_score(labels_flat, pred_flat, zero_division=0, average=\"weighted\")\n",
    "        \n",
    "        return precision, recall, f1, accuracy\n",
    "\n",
    "    def train(self, data_loader, optimizer):\n",
    "        self.model.train()\n",
    "        total_recall = 0\n",
    "        total_precision = 0\n",
    "        total_f1 = 0\n",
    "        total_accuracy = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (reviews, labels) in enumerate(tqdm(data_loader)):\n",
    "            self.model.zero_grad()\n",
    "            reviews, labels = reviews.to(self.device), labels.to(self.device)\n",
    "            outputs = self.model(reviews, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = outputs.logits.detach().cpu().numpy()\n",
    "            labels_np = labels.cpu().numpy()\n",
    "            precision, recall, f1, accuracy = self.get_performance_metrics(preds, labels_np)\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_accuracy += accuracy\n",
    "            total_f1 += f1\n",
    "\n",
    "        precision = total_precision/len(data_loader)\n",
    "        recall = total_recall/len(data_loader)\n",
    "        f1 = total_f1/len(data_loader)\n",
    "        accuracy = total_accuracy/len(data_loader)\n",
    "        loss = total_loss/len(data_loader)\n",
    "\n",
    "        return precision, recall, f1, accuracy, loss\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the validation dataset.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        total_recall = 0\n",
    "        total_precision = 0\n",
    "        total_f1 = 0\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (answers, labels) in tqdm(data_loader):\n",
    "                self.model.zero_grad()\n",
    "                answers, labels = answers.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(answers, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                preds = outputs.logits.detach().cpu().numpy()\n",
    "                labels_np = labels.cpu().numpy()\n",
    "                precision, recall, f1, accuracy = self.get_performance_metrics(preds, labels_np)\n",
    "                total_precision += precision\n",
    "                total_recall += recall\n",
    "                total_accuracy += accuracy\n",
    "                total_f1 += f1\n",
    "\n",
    "            precision = total_precision/len(data_loader)\n",
    "            recall = total_recall/len(data_loader)\n",
    "            f1 = total_f1/len(data_loader)\n",
    "            accuracy = total_accuracy/len(data_loader)\n",
    "            loss = total_loss/len(data_loader)\n",
    "\n",
    "        return precision, recall, f1, accuracy, loss\n",
    "    \n",
    "    def save_transformer(self):\n",
    "        self.model.save_pretrained(self.save_path)\n",
    "        self.tokenizer.save_pretrained(self.save_path)\n",
    "\n",
    "    def execute(self):\n",
    "        last_best = 0\n",
    "        train_data_loader = self.train_dataset.get_data_loaders(self.batch_size)\n",
    "        val_data_loader = self.eval_dataset.get_data_loaders(self.batch_size)\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr = 3e-5, eps = 1e-8)\n",
    "        self.set_training_parameters()\n",
    "        for epoch_i in range(0, self.epochs):\n",
    "            train_precision, train_recall, train_f1, train_accuracy, train_loss = self.train(train_data_loader, optimizer)\n",
    "            print(f'Epoch {epoch_i + 1}: train_loss: {train_loss:.4f} train_precision: {train_precision:.4f} train_recall: {train_recall:.4f} train_accuracy: {train_accuracy:.4f}train_f1: {train_f1:.4f}')\n",
    "            val_precision, val_recall, val_f1, val_accuracy, val_loss = self.evaluate(val_data_loader)\n",
    "            print(f'Epoch {epoch_i + 1}: val_loss: {val_loss:.4f} val_precision: {val_precision:.4f} val_recall: {val_recall:.4f} val_accuracy: {val_accuracy:.4f} val_f1: {val_f1:.4f}')\n",
    "\n",
    "            if val_f1 > last_best:\n",
    "                print(\"Saving model..\")\n",
    "                self.save_transformer()\n",
    "                last_best = val_f1\n",
    "                print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training type set to: fully_frozen\n",
      "Processing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1301/1301 [00:00<00:00, 5120.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 4346.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training type set to: fully_frozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 82/82 [00:01<00:00, 51.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.6945 train_precision: 0.5348 train_recall: 0.4965 train_accuracy: 0.4965train_f1: 0.4899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 49.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss: 0.6921 val_precision: 0.5804 val_recall: 0.5471 val_accuracy: 0.5471 val_f1: 0.5338\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 82/82 [00:01<00:00, 51.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6915 train_precision: 0.5630 train_recall: 0.5280 train_accuracy: 0.5280train_f1: 0.5265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 50.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: val_loss: 0.6910 val_precision: 0.6517 val_recall: 0.6140 val_accuracy: 0.6140 val_f1: 0.6064\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 82/82 [00:01<00:00, 52.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6901 train_precision: 0.5750 train_recall: 0.5323 train_accuracy: 0.5323train_f1: 0.5296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 51.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: val_loss: 0.6901 val_precision: 0.6623 val_recall: 0.6393 val_accuracy: 0.6393 val_f1: 0.6337\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    big5category=\"OPN\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    epochs=EPOCHS,\n",
    "    save_path=SAVE_PATH + '_fully_frozen',\n",
    "    training_type='fully_frozen',\n",
    "    train_data=train_data,\n",
    "    eval_data=eval_data\n",
    ")\n",
    "trainer.execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    big5category=\"OPN\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    epochs=EPOCHS,\n",
    "    save_path=SAVE_PATH + 'top_4_training',\n",
    "    training_type='top_4_training',\n",
    "    train_data=train_data,\n",
    "    eval_data=eval_data\n",
    ")\n",
    "trainer.execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    big5category=\"OPN\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    epochs=EPOCHS,\n",
    "    save_path=SAVE_PATH + 'bottom_4_training',\n",
    "    training_type='bottom_4_training',\n",
    "    train_data=train_data,\n",
    "    eval_data=eval_data\n",
    ")\n",
    "trainer.execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    big5category=\"OPN\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    epochs=EPOCHS,\n",
    "    save_path=SAVE_PATH + 'all_training',\n",
    "    training_type='all_training',\n",
    "    train_data=train_data,\n",
    "    eval_data=eval_data\n",
    ")\n",
    "trainer.execute()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
