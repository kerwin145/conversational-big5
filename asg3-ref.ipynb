{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can create a NLP assignment3 folder in your Google drive upload the data folder there\n",
    "# base_dir = \"drive/MyDrive/ which went over training a diffusion model to generate images, \"\n",
    "base_dir = \"/content/drive/MyDrive/CSE_354_HW3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Constants in the file**\n",
    "---\n",
    "\n",
    "The code block below contains a few constants.\n",
    "\n",
    "\n",
    "1.   **BATCH_SIZE**: The batch size input to the models. This has been set to 16 and should not be changed. In case you encounter any CUDA - out of memory errors while training your models, this value may be reduced from 16. But please mention this in your submission report.\n",
    "2.   **EPOCHS**: The number of epochs to train your model. This should not be changed.\n",
    "3. **TEST_PATH**: This is the path to the test_data.csv file. For example, \"/content/drive/MyDrive/CSE_354_HW3/test_data.csv\".\n",
    "4. **TRAIN_PATH**: This is the path to the train_data.csv file. For example, \"/content/drive/MyDrive/CSE_354_HW3/train_data.csv\".\n",
    "5. **VAL_PATH**: This is the path to the val_data.csv file. For example, \"/content/drive/MyDrive/CSE_354_HW3/val_data.csv\".\n",
    "6. **SAVE_PATH**: This is the path to directory your model will be saved. For example, \"/content/drive/MyDrive/CSE_354_HW3/DistilBERT\". Note: This path will be altered further down in the code by appending the name of the kind of DistilBERT model you train as per your experiments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "# constants which can be changed\n",
    "TEST_PATH = \"data/test_data.csv\"\n",
    "TRAIN_PATH = \"data/train_data.csv\"\n",
    "VAL_PATH = \"data/val_data.csv\"\n",
    "# Models are stored in this path\n",
    "SAVE_PATH = \"data/DistilBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "  dataset = pd.read_csv(path)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_dataset(TRAIN_PATH)\n",
    "val_data = load_dataset(VAL_PATH)\n",
    "test_data = load_dataset(TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem 1 (Initialize the Model Class)**\n",
    "---\n",
    "\n",
    "In the code block below, you would need to load a pre-trained DistilBERT model and it's tokenizer using Hugging Face's library. The model you would need to load is called \"distilbert-base-uncased\". It would also need to have a classifier head on top which has the *num_classes* as the output shape of the model (in this case it is going to be 2). Please write your code between the given TODO block.\n",
    "\n",
    "More about the model and how to load it can be read at - https://huggingface.co/distilbert-base-uncased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillBERT():\n",
    "\n",
    "  def __init__(self, model_name='distilbert-base-uncased', num_classes=2):\n",
    "    # TODO(students): start\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = num_classes)\n",
    "    # TODO(students): end\n",
    "\n",
    "  def get_tokenizer_and_model(self):\n",
    "    return self.model, self.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem 2 (Initialize the Dataloader Class)**\n",
    "---\n",
    "\n",
    "The code block below takes your data_frame and the tokenizer you loaded in the previous block and generates the DataLoader object for it. You would need to implement a part of the tokenize_data method. This method takes the given data and generates a list of token IDs for a given review along with it's label. You would need to use the tokenizer to generated the token encoded values for each review. **Please ensure that the max_length of an encoded review is 512 tokens.**\n",
    "\n",
    "You would also need to convert the labels to a corresponding numerical class using the label_dict dictionary. Please write your code between the given TODO block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader(Dataset):\n",
    "\n",
    "  def __init__(self, data, tokenizer):\n",
    "    self.data = data\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def tokenize_data(self):\n",
    "    print(\"Processing data..\")\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    label_dict = {'positive': 1, 'negative': 0}\n",
    "\n",
    "    review_list = self.data['review'].to_list()\n",
    "    label_list = self.data['sentiment'].to_list()\n",
    "\n",
    "    for (review, label) in tqdm(zip(review_list, label_list), total=len(review_list)):\n",
    "      # TODO(students): start\n",
    "      encoded_review = self.tokenizer.encode(review, max_length=512, truncation=True)\n",
    "      tokens.append(torch.tensor(encoded_review))\n",
    "      labels.append(label_dict[label])\n",
    "      # TODO(students): end\n",
    "\n",
    "    tokens = pad_sequence(tokens, batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    dataset = TensorDataset(tokens, labels)\n",
    "    return dataset\n",
    "\n",
    "  def get_data_loaders(self, batch_size=32, shuffle=True):\n",
    "    processed_dataset = self.tokenize_data()\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        processed_dataset,\n",
    "        shuffle=shuffle,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem 3 (Training Function)**\n",
    "---\n",
    "\n",
    "The class below provides method to train a given model. It takes a dictionary with the following parameters:\n",
    "\n",
    "\n",
    "\n",
    "1.   device: The device to run the model on.\n",
    "2.   train_data: The train_data dataframe.\n",
    "3.   val_data: The val_data dataframe.\n",
    "4.   batch_size: The batch_size which is input to the model.\n",
    "5.   epochs: The number of epochs to train the model.\n",
    "6.   training_type: The type of training that your model will be undergoing. This can take four values - 'frozen_embeddings', 'top_2_training', 'top_4_training' and 'all_training'.\n",
    "\n",
    "#### Problem 3(a)\n",
    "\n",
    "Your first problem here would be to implement the set_training_parameters() method. In this method you will select the layers of your model to train based on the training_type. **Note: By default the Hugging Face DistilBERT has 6 layers.**\n",
    "\n",
    "1. fully_frozen: This setting is supposed to train the DistilBERT model with all parameters except the classifier 'frozen' or not trainable.\n",
    "2. top_4_training: This setting is supposed to train just the final four layers of DistilBERT (layer 5, layer 4, layer 3 and layer 2). All other layers before these would need to be frozen. Do not freeze the classifier head parameters.\n",
    "3. bottom_4_training: This setting is supposed to train just the bottom four layers of DistilBERT (layer 3, layer 2, layer 1 and layer 0). All other layers after these would need to be frozen. Do not freeze the classifier head parameters.\n",
    "4. all_training: All layers of DistilBERT would need to trained.\n",
    "\n",
    "Please write your code between the given TODO block.\n",
    "\n",
    "**Note: The classifier head on top of the final DistilBERT layer would always need to be trained, please do not freeze that layer.**\n",
    "\n",
    "#### Problem 3(b)\n",
    "\n",
    "Your second problem would be to implement a single training step in the given loop inside the train() method. You would need to pass the review and label in the given batch to the model, take the output and compute the Precision, Recall and F1 for that batch using the get_performance_metrics() method. You would also need to propagate the loss backwards to the model and update the given optimizer's parameters.\n",
    "\n",
    "Please write your code between the given TODO block.\n",
    "\n",
    "#### Problem 3(c)\n",
    "\n",
    "Your second problem would be to implement a single validation step in the given loop inside the eval() method. You would need to pass the review and label in the given batch to the model, take the output and compute the Precision, Recall and F1 for that batch using the get_performance_metrics() method. You would need to ensure that the loss is not propagated backwards.\n",
    "\n",
    "Please write your code between the given TODO block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "  def __init__(self, options):\n",
    "    self.device = options['device']\n",
    "    self.train_data = options['train_data']\n",
    "    self.val_data = options['val_data']\n",
    "    self.batch_size = options['batch_size']\n",
    "    self.epochs = options['epochs']\n",
    "    self.save_path = options['save_path']\n",
    "    self.training_type = options['training_type']\n",
    "    transformer = DistillBERT()\n",
    "    self.model, self.tokenizer = transformer.get_tokenizer_and_model()\n",
    "    self.model.to(self.device)\n",
    "\n",
    "  def get_performance_metrics(self, preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    precision = precision_score(labels_flat, pred_flat, zero_division=0)\n",
    "    recall = recall_score(labels_flat, pred_flat, zero_division=0)\n",
    "    f1 = f1_score(labels_flat, pred_flat, zero_division=0)\n",
    "    return precision, recall, f1\n",
    "\n",
    "  def set_training_parameters(self):\n",
    "    # TODO(students): start\n",
    "\n",
    "    # by default freeze all parameters (assume 'fully_frozen')\n",
    "    for param in self.model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # classifier params should always be trainable\n",
    "    for param in self.model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if self.training_type == 'top_4_training':\n",
    "        # Unfreeze the final four transformer layers (layer 5 to layer 2)\n",
    "        for layer in self.model.distilbert.transformer.layer[2:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    elif self.training_type == 'bottom_4_training':\n",
    "        # Unfreeze the first four transformer layers (layer 0 to layer 3)\n",
    "        for layer in self.model.distilbert.transformer.layer[:4]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    elif self.training_type == 'all_training':\n",
    "        # Unfreeze all transformer layers\n",
    "        for layer in self.model.distilbert.transformer.layer:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    # TODO(students): end\n",
    "\n",
    "  def train(self, data_loader, optimizer):\n",
    "    self.model.train()\n",
    "    total_recall = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (reviews, labels) in enumerate(tqdm(data_loader)):\n",
    "      self.model.zero_grad()\n",
    "      # TODO(students): start\n",
    "      reviews, labels = reviews.to(self.device), labels.to(self.device)\n",
    "      outputs = self.model(reviews, labels=labels)\n",
    "      loss = outputs.loss\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      preds = outputs.logits.detach().cpu().numpy()\n",
    "      labels_np = labels.cpu().numpy()\n",
    "      precision, recall, f1 = self.get_performance_metrics(preds, labels_np)\n",
    "      total_precision += precision\n",
    "      total_recall += recall\n",
    "      total_f1 += f1\n",
    "      # TODO(students): end\n",
    "\n",
    "    precision = total_precision/len(data_loader)\n",
    "    recall = total_recall/len(data_loader)\n",
    "    f1 = total_f1/len(data_loader)\n",
    "    loss = total_loss/len(data_loader)\n",
    "\n",
    "    return precision, recall, f1, loss\n",
    "\n",
    "  def eval(self, data_loader):\n",
    "    self.model.eval()\n",
    "    total_recall = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for (reviews, labels) in tqdm(data_loader):\n",
    "        # TODO(students): start\n",
    "        reviews, labels = reviews.to(self.device), labels.to(self.device)\n",
    "        outputs = self.model(reviews, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = outputs.logits.detach().cpu().numpy()\n",
    "        labels_np = labels.cpu().numpy()\n",
    "        precision, recall, f1 = self.get_performance_metrics(preds, labels_np)\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "        # TODO(students): end\n",
    "\n",
    "    precision = total_precision/len(data_loader)\n",
    "    recall = total_recall/len(data_loader)\n",
    "    f1 = total_f1/len(data_loader)\n",
    "    loss = total_loss/len(data_loader)\n",
    "\n",
    "    return precision, recall, f1, loss\n",
    "\n",
    "  def save_transformer(self):\n",
    "    self.model.save_pretrained(self.save_path)\n",
    "    self.tokenizer.save_pretrained(self.save_path)\n",
    "\n",
    "  def execute(self):\n",
    "    last_best = 0\n",
    "    train_dataset = DatasetLoader(self.train_data, self.tokenizer)\n",
    "    train_data_loader = train_dataset.get_data_loaders(self.batch_size)\n",
    "    val_dataset = DatasetLoader(self.val_data, self.tokenizer)\n",
    "    val_data_loader = val_dataset.get_data_loaders(self.batch_size)\n",
    "    optimizer = torch.optim.AdamW(self.model.parameters(), lr = 3e-5, eps = 1e-8)\n",
    "    self.set_training_parameters()\n",
    "    for epoch_i in range(0, self.epochs):\n",
    "      train_precision, train_recall, train_f1, train_loss = self.train(train_data_loader, optimizer)\n",
    "      print(f'Epoch {epoch_i + 1}: train_loss: {train_loss:.4f} train_precision: {train_precision:.4f} train_recall: {train_recall:.4f} train_f1: {train_f1:.4f}')\n",
    "      val_precision, val_recall, val_f1, val_loss = self.eval(val_data_loader)\n",
    "      print(f'Epoch {epoch_i + 1}: val_loss: {val_loss:.4f} val_precision: {val_precision:.4f} val_recall: {val_recall:.4f} val_f1: {val_f1:.4f}')\n",
    "\n",
    "      if val_f1 > last_best:\n",
    "        print(\"Saving model..\")\n",
    "        self.save_transformer()\n",
    "        last_best = val_f1\n",
    "        print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem statement**\n",
    "---\n",
    "In this homework, you will be using pre-trained language models to predict the sentiment of a given movie review.\n",
    "\n",
    "\n",
    "The dataset, which is given to you, is sampled from the [IMDB dataset of 50k movie reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews). The sentences are sampled to a smaller set to help with quicker computation on Colab. The data contains a review and an associated label for the sentiment of that review. The label can either be *positive* or *negative*. You have been given three files - train_data.csv, val_data.csv and test_data.csv. The training data will be used to fine-tune the language model, the val data will be used to select the best model while training and finally the test data will give the model's final performance on the data.\n",
    "\n",
    "To perform this task you will be using a pre-trained DistilBERT model. DistilBERT is a BERT based language model. It is less than half the size of BERT, but its retains much of BERT's capabilities for many tasks and is 1.6X faster. You can read more about DistilBERT - https://arxiv.org/abs/1910.01108.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.37.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experiment 1**\n",
    "---\n",
    "Training your DistilBERT with frozen embeddings and layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef9bc08613e4919a1a2b19c82e32131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd38854febce4f7d81576013b0ec5251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff82222a553a44409af52e3dddefb45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb80e7d440a45658b51ef95e14cd4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee777d688bd04f5c96e061a194aec4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5130/5130 [00:05<00:00, 880.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 938.41it/s]\n",
      "  0%|          | 0/321 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "100%|██████████| 321/321 [01:26<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.6962 train_precision: 0.4827 train_recall: 0.5570 train_f1: 0.4991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss: 0.6932 val_precision: 0.5248 val_recall: 0.9704 val_f1: 0.6667\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:33<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6933 train_precision: 0.5065 train_recall: 0.6774 train_f1: 0.5658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: val_loss: 0.6922 val_precision: 0.5352 val_recall: 0.9390 val_f1: 0.6687\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:34<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6933 train_precision: 0.5204 train_recall: 0.6516 train_f1: 0.5598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: val_loss: 0.6909 val_precision: 0.5273 val_recall: 0.9888 val_f1: 0.6764\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options['batch_size'] = BATCH_SIZE\n",
    "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options['train_data'] = train_data\n",
    "options['val_data'] = val_data\n",
    "options['save_path'] = SAVE_PATH + '_fully_frozen'\n",
    "options['epochs'] = EPOCHS\n",
    "options['training_type'] = 'fully_frozen'\n",
    "trainer = Trainer(options)\n",
    "trainer.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experiment 2**\n",
    "---\n",
    "Training your DistilBERT with bottom 4 layers being trained. This should take around 5-6 minutes per epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5130/5130 [00:10<00:00, 495.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 346.07it/s]\n",
      "100%|██████████| 321/321 [03:51<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.4545 train_precision: 0.7932 train_recall: 0.7470 train_f1: 0.7429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss: 0.2687 val_precision: 0.9026 val_recall: 0.9073 val_f1: 0.9013\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [03:53<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.2566 train_precision: 0.9151 train_recall: 0.9089 train_f1: 0.9031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: val_loss: 0.2380 val_precision: 0.9356 val_recall: 0.9014 val_f1: 0.9110\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [03:53<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.1539 train_precision: 0.9551 train_recall: 0.9495 train_f1: 0.9490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: val_loss: 0.2446 val_precision: 0.9391 val_recall: 0.8610 val_f1: 0.8927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options['batch_size'] = BATCH_SIZE\n",
    "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options['train_data'] = train_data\n",
    "options['val_data'] = val_data\n",
    "options['save_path'] = SAVE_PATH + '_bottom_4_training'\n",
    "options['epochs'] = EPOCHS\n",
    "options['training_type'] = 'bottom_4_training'\n",
    "trainer = Trainer(options)\n",
    "trainer.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experiment 3**\n",
    "---\n",
    "Training your DistilBERT with only top 4 layers being trained. This should take around 6 minutes per epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5130/5130 [00:05<00:00, 889.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 961.50it/s]\n",
      "100%|██████████| 321/321 [03:18<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.4231 train_precision: 0.7917 train_recall: 0.8009 train_f1: 0.7720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss: 0.2383 val_precision: 0.8581 val_recall: 0.9494 val_f1: 0.8953\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [03:18<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.2199 train_precision: 0.9168 train_recall: 0.9098 train_f1: 0.9072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: val_loss: 0.2235 val_precision: 0.8982 val_recall: 0.9350 val_f1: 0.9126\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [03:18<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.1376 train_precision: 0.9516 train_recall: 0.9502 train_f1: 0.9472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: val_loss: 0.2562 val_precision: 0.8891 val_recall: 0.9674 val_f1: 0.9236\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options['batch_size'] = BATCH_SIZE\n",
    "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options['train_data'] = train_data\n",
    "options['val_data'] = val_data\n",
    "options['save_path'] = SAVE_PATH + '_top_4_training'\n",
    "options['epochs'] = EPOCHS\n",
    "options['training_type'] = 'top_4_training'\n",
    "trainer = Trainer(options)\n",
    "trainer.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experiment 4**\n",
    "---\n",
    "Training your DistilBERT with all layers being trained. This should take around 8 minutes per epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5130/5130 [00:06<00:00, 784.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 412.37it/s]\n",
      "100%|██████████| 321/321 [04:13<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.4255 train_precision: 0.7906 train_recall: 0.8088 train_f1: 0.7708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss: 0.3241 val_precision: 0.8085 val_recall: 0.9737 val_f1: 0.8775\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [04:13<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.2081 train_precision: 0.9270 train_recall: 0.9190 train_f1: 0.9161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: val_loss: 0.2458 val_precision: 0.8727 val_recall: 0.9935 val_f1: 0.9257\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [04:14<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.1216 train_precision: 0.9558 train_recall: 0.9590 train_f1: 0.9540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: val_loss: 0.1944 val_precision: 0.9178 val_recall: 0.9646 val_f1: 0.9363\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options['batch_size'] = BATCH_SIZE\n",
    "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options['train_data'] = train_data\n",
    "options['val_data'] = val_data\n",
    "options['save_path'] = SAVE_PATH + '_all_training'\n",
    "options['epochs'] = EPOCHS\n",
    "options['training_type'] = 'all_training'\n",
    "trainer = Trainer(options)\n",
    "trainer.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem 4 (Test Function)**\n",
    "---\n",
    "\n",
    "The class below provides method to test a given model. It takes a dictionary with the following parameters:\n",
    "\n",
    "\n",
    "\n",
    "1.   device: The device to run the model on.\n",
    "2.   test_data: The test_data dataframe.\n",
    "3.   batch_size: The batch_size which is input to the model.\n",
    "4.   save_path: The directory of your saved model.\n",
    "\n",
    "You would need to implement a single test step in the given loop inside the test() method. You would need to pass the review and label in the given batch to the model, take the output and compute the Precision, Recall and F1 for that batch using the get_performance_metrics() method. You would need to ensure that the loss is not propagated backwards.\n",
    "\n",
    "Please write your code between the given TODO block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester():\n",
    "\n",
    "  def __init__(self, options):\n",
    "    self.save_path = options['save_path']\n",
    "    self.device = options['device']\n",
    "    self.test_data = options['test_data']\n",
    "    self.batch_size = options['batch_size']\n",
    "    transformer = DistillBERT(self.save_path)\n",
    "    self.model, self.tokenizer = transformer.get_tokenizer_and_model()\n",
    "    self.model.to(self.device)\n",
    "\n",
    "  def get_performance_metrics(self, preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    precision = precision_score(labels_flat, pred_flat, zero_division=0)\n",
    "    recall = recall_score(labels_flat, pred_flat, zero_division=0)\n",
    "    f1 = f1_score(labels_flat, pred_flat, zero_division=0)\n",
    "    return precision, recall, f1\n",
    "\n",
    "  def test(self, data_loader):\n",
    "    self.model.eval()\n",
    "    total_recall = 0\n",
    "    total_precision = 0\n",
    "    total_f1 = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for (reviews, labels) in tqdm(data_loader):\n",
    "        # TODO(students): start\n",
    "        reviews, labels = reviews.to(self.device), labels.to(self.device)\n",
    "        outputs = self.model(reviews, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = outputs.logits.detach().cpu().numpy()\n",
    "        labels_np = labels.cpu().numpy()\n",
    "        precision, recall, f1 = self.get_performance_metrics(preds, labels_np)\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "        # TODO(students): end\n",
    "\n",
    "    precision = total_precision/len(data_loader)\n",
    "    recall = total_recall/len(data_loader)\n",
    "    f1 = total_f1/len(data_loader)\n",
    "    loss = total_loss/len(data_loader)\n",
    "\n",
    "    return precision, recall, f1, loss\n",
    "\n",
    "  def execute(self):\n",
    "    test_dataset = DatasetLoader(self.test_data, self.tokenizer)\n",
    "    test_data_loader = test_dataset.get_data_loaders(self.batch_size)\n",
    "\n",
    "    test_precision, test_recall, test_f1, test_loss = self.test(test_data_loader)\n",
    "\n",
    "    print()\n",
    "    print(f'test_loss: {test_loss:.4f} test_precision: {test_precision:.4f} test_recall: {test_recall:.4f} test_f1: {test_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experiment 5**\n",
    "---\n",
    "Testing your DistilBERT trained with frozen embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:00<00:00, 1033.93it/s]\n",
      "100%|██████████| 38/38 [00:10<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_loss: 0.6919 test_precision: 0.4945 test_recall: 0.9137 test_f1: 0.6313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options['batch_size'] = BATCH_SIZE\n",
    "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options['test_data'] = test_data\n",
    "options['save_path'] = SAVE_PATH + '_fully_frozen'\n",
    "tester = Tester(options)\n",
    "tester.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experiment 6**\n",
    "---\n",
    "Testing your DistilBERT trained with all layers frozen except the final two layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:00<00:00, 941.00it/s]\n",
      "100%|██████████| 38/38 [00:10<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_loss: 0.2593 test_precision: 0.9187 test_recall: 0.8521 test_f1: 0.8762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options['batch_size'] = BATCH_SIZE\n",
    "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options['test_data'] = test_data\n",
    "options['save_path'] = SAVE_PATH + '_bottom_4_training'\n",
    "tester = Tester(options)\n",
    "tester.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experiment 7**\n",
    "---\n",
    "Testing your DistilBERT trained with all layers frozen except the final four layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:00<00:00, 911.13it/s]\n",
      "100%|██████████| 38/38 [00:10<00:00,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_loss: 0.3100 test_precision: 0.8770 test_recall: 0.8854 test_f1: 0.8772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options['batch_size'] = BATCH_SIZE\n",
    "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options['test_data'] = test_data\n",
    "options['save_path'] = SAVE_PATH + '_top_4_training'\n",
    "tester = Tester(options)\n",
    "tester.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experiment 8**\n",
    "---\n",
    "Testing your DistilBERT trained with all layers trainable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:00<00:00, 1015.37it/s]\n",
      "100%|██████████| 38/38 [00:10<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_loss: 0.3065 test_precision: 0.8626 test_recall: 0.9133 test_f1: 0.8724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options['batch_size'] = BATCH_SIZE\n",
    "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options['test_data'] = test_data\n",
    "options['save_path'] = SAVE_PATH + '_all_training'\n",
    "tester = Tester(options)\n",
    "tester.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Results**\n",
    "---\n",
    "\n",
    "Answer the following questions based on the analyses you have performed above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Briefly explain your code implementations for each TO-DO task.\n",
    "#### Problem 1\n",
    "The following code\n",
    "```\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = num_classes)\n",
    "```\n",
    " used the functions from the import statement:\n",
    "```\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "```\n",
    "The values model_name and num_labels passed in to these functions were given as parameters to the init__ function\n",
    "\n",
    "#### Problem 2\n",
    "```\n",
    "    for (review, label) in tqdm(zip(review_list, label_list), total=len(review_list)):\n",
    "      # TODO(students): start\n",
    "      encoded_review = self.tokenizer.encode(review, max_length=512, truncation=True)\n",
    "      tokens.append(torch.tensor(encoded_review))\n",
    "      labels.append(label_dict[label])\n",
    "      # TODO(students): end\n",
    "```\n",
    "\n",
    "Uses the tokenizer from problem 1, along with the params of review, and max length (which apparently required truncation = True)\n",
    "The following lines appended to the initialized ```tokens``` and ```labels``` arrays from a few lines before\n",
    "\n",
    "#### Problem 3\n",
    "```\n",
    "  def set_training_parameters(self):\n",
    "    # TODO(students): start\n",
    "\n",
    "    # by default freeze all parameters (assume 'fully_frozen')\n",
    "    for param in self.model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # classifier params should always be trainable\n",
    "    for param in self.model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if self.training_type == 'top_4_training':\n",
    "        # Unfreeze the final four transformer layers (layer 5 to layer 2)\n",
    "        for layer in self.model.distilbert.transformer.layer[2:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    elif self.training_type == 'bottom_4_training':\n",
    "        # Unfreeze the first four transformer layers (layer 0 to layer 3)\n",
    "        for layer in self.model.distilbert.transformer.layer[:4]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    elif self.training_type == 'all_training':\n",
    "        # Unfreeze all transformer layers\n",
    "        for layer in self.model.distilbert.transformer.layer:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    # TODO(students): end\n",
    "  ```\n",
    "\n",
    "  Above are if-statements for the different training types. I made fully_frozen the default, setting requires_grad of the model's params to false, thus preventing training from occuring on these params.\n",
    "\n",
    "  I also made all classifier params trainable for all the four training options, as we will always need to train the classifier layer.\n",
    "\n",
    "  In the if statmenets, we use array slicing to turn on training for the relevant layers of the BERT pretrained model\n",
    "\n",
    "  #### Problem 3b\n",
    "  ```    \n",
    "  for batch_idx, (reviews, labels) in enumerate(tqdm(data_loader)):\n",
    "      self.model.zero_grad()\n",
    "      # TODO(students): start\n",
    "      reviews, labels = reviews.to(self.device), labels.to(self.device)\n",
    "      outputs = self.model(reviews, labels=labels)\n",
    "      loss = outputs.loss\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      preds = outputs.logits.detach().cpu().numpy()\n",
    "      labels_np = labels.cpu().numpy()\n",
    "      precision, recall, f1 = self.get_performance_metrics(preds, labels_np)\n",
    "      total_precision += precision\n",
    "      total_recall += recall\n",
    "      total_f1 += f1\n",
    "      # TODO(students): end\n",
    "  ```\n",
    "  A single training step is done by first moving reviews and labels to our device (probably the GPU). We then pass our reviews and labels into the model to produce outputs, and extract losses. Loss.backward() computes the gradients and optimizer.step() applies the gradients\n",
    "\n",
    "  The next section extracts the performance metrics. Excracting predictions from the outputs (outputs.logits), then moving the predictions and the labels to the cpu and converting to numpy objects, we can finally accumulate the metrics.\n",
    "\n",
    "  #### Problem 3c\n",
    "  ```\n",
    "        for (reviews, labels) in tqdm(data_loader):\n",
    "        # TODO(students): start\n",
    "        reviews, labels = reviews.to(self.device), labels.to(self.device)\n",
    "        outputs = self.model(reviews, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = outputs.logits.detach().cpu().numpy()\n",
    "        labels_np = labels.cpu().numpy()\n",
    "        precision, recall, f1 = self.get_performance_metrics(preds, labels_np)\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "        # TODO(students): end\n",
    "```\n",
    "Very similar to 3b, except it is running in the context of torch.no_grad(), which also means we don't calculate and apply gradients. We do keep track of total loss still for performance monitoring.\n",
    "\n",
    "#### Problem 4\n",
    "  The code for this is identical to problem 3c, but we are evaluating on the test data rather than the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO [STUDENT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A table containing the precision, recall and F1 scores of each DistilBERT model during validation and testing.\n",
    "\n",
    "### Validation\n",
    "\n",
    "| Category           | Loss   | Precision | Recall | F1 Score |\n",
    "|--------------------|--------|-----------|--------|----------|\n",
    "| Fully Frozen       | 0.6907 | 0.5753    | 0.5227 | 0.5245   |\n",
    "| Bottom 4 Training  | 0.2446 | 0.9391    | 0.8610 | 0.8927   |\n",
    "| Top 4 Training     | 0.2138 | 0.9132    | 0.9381 | 0.9229   |\n",
    "| All Training       | 0.1944 | 0.9178    | 0.9646 | 0.9363   |\n",
    "\n",
    "\n",
    "\n",
    "### Test\n",
    "\n",
    "| Category           | Loss   | Precision | Recall | F1 Score |\n",
    "|--------------------|--------|-----------|--------|----------|\n",
    "| Fully Frozen       | 0.6919 | 0.4945    | 0.9137 | 0.6313   |\n",
    "| Bottom 4 Training  | 0.2593 | 0.9187    | 0.8521 | 0.8762   |\n",
    "| Top 4 Training     | 0.3100 | 0.8770    | 0.8854 | 0.8772   |\n",
    "| All Training       | 0.3065 | 0.8626    | 0.9133 | 0.8724   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO [STUDENT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. An analysis explaining your understanding of the impact freezing/training different layers has on the model's performance.\n",
    "\n",
    "Firstly, fully frozen performs the worst in both validation and testing. It may be that changes to the text transformer (separate from the classifier) is needed. This may be because newer context related to the reviews we are classfying need to be learned; ie. training the classifier is not enough.\n",
    "\n",
    "The other training all performed similarly, though it is interesting to note that Top 4 training performed better in both validation and test, maybe because the params in the bottom layer still held key \"generalization\" power for text.\n",
    "\n",
    "One last thing worst noting is that training all layers performed the best in training but worse in test, compared to the Top 4 training metric. It may be because training all the transformers' parameters caused the model to lose ability to understand text in a general since, sort of akin to overfitting. Perhaps all training could perform better than top 4 training if there was a larger amount of training data that is also diverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO [STUDENT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Submission guidelines**\n",
    "---\n",
    "You would need to submit the following files:\n",
    "\n",
    "\n",
    "1.   `NLP_HW3.ipynb` - This jupyter notebook. It will also work as your report, so please add description to code wherever required. Also make sure to write your analyses outcomes in the RESULTS section above.\n",
    "2.   `gdrive_link.txt` - Should contain a wgetable to a folder that contains your four DistilBERT models. Please make sure you provide the necessary permissions.\n",
    "\n",
    "**Colab design credit**: Dhruv Verma, Yash Kumar Lal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
